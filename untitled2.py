# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oPQivwD3KZpcQMBRvaqi8RZA3Eo7IAnu
"""

# Install required libraries
!pip install textblob
import nltk
nltk.download('brown')
nltk.download('punkt')

from textblob import TextBlob

def analyze_sentiment(text):
    """Analyze sentiment of the given text"""
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity

    if polarity > 0:
        sentiment = "Positive ðŸ˜Š"
    elif polarity < 0:
        sentiment = "Negative ðŸ˜ž"
    else:
        sentiment = "Neutral ðŸ˜"

    return sentiment, polarity

# Enter your text here
text = input("Enter text to analyze: ")

sentiment, score = analyze_sentiment(text)

print(f"\nðŸ“Š Sentiment: {sentiment}")
print(f"ðŸ“ˆ Score: {score:.2f}")
print(f"   (Range: -1.0 to +1.0)")

# Test with multiple examples
examples = [
    "I love this product! It's amazing!",
    "This is terrible and disappointing.",
    "The weather is okay today.",
    "Best day ever! So happy!",
    "I hate waiting in long lines."
]

print("Testing multiple examples:\n")
for text in examples:
    sentiment, score = analyze_sentiment(text)
    print(f"Text: {text}")
    print(f"Sentiment: {sentiment} | Score: {score:.2f}\n")











# Install required libraries
!pip install deepface opencv-python-headless tf-keras

from deepface import DeepFace
import cv2
from google.colab.patches import cv2_imshow
from google.colab import files
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import numpy as np
from PIL import Image as PILImage
import io

def take_photo(filename='photo.jpg', quality=0.8):
    """Take a photo using webcam"""
    js = Javascript('''
        async function takePhoto(quality) {
            const div = document.createElement('div');
            const capture = document.createElement('button');
            capture.textContent = 'Capture';
            div.appendChild(capture);

            const video = document.createElement('video');
            video.style.display = 'block';
            const stream = await navigator.mediaDevices.getUserMedia({video: true});

            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;
            await video.play();

            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

            await new Promise((resolve) => capture.onclick = resolve);

            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            stream.getVideoTracks()[0].stop();
            div.remove();
            return canvas.toDataURL('image/jpeg', quality);
        }
    ''')
    display(js)

    data = eval_js('takePhoto({})'.format(quality))
    binary = b64decode(data.split(',')[1])

    with open(filename, 'wb') as f:
        f.write(binary)

    return filename

def analyze_emotion(image_path):
    """Analyze emotion from image"""
    try:
        # Analyze the image
        result = DeepFace.analyze(image_path, actions=['emotion'], enforce_detection=False)

        # Get dominant emotion
        if isinstance(result, list):
            result = result[0]

        dominant_emotion = result['dominant_emotion']
        emotions = result['emotion']

        return dominant_emotion, emotions
    except Exception as e:
        return None, str(e)

def display_results(image_path, dominant_emotion, emotions):
    """Display image and emotion results"""
    # Read and display image
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Add text to image
    cv2.putText(img_rgb, f"Emotion: {dominant_emotion}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

    # Display
    cv2_imshow(img_rgb)

    print("\n" + "="*50)
    print(f"ðŸŽ­ Dominant Emotion: {dominant_emotion.upper()}")
    print("="*50)
    print("\nAll Emotions Detected:")
    print("-"*50)

    # Sort emotions by percentage
    sorted_emotions = sorted(emotions.items(), key=lambda x: x[1], reverse=True)

    for emotion, score in sorted_emotions:
        print(f"{emotion.capitalize():12} : {score:.2f}%")

# OPTION 1: Capture photo from webcam
print("Click 'Capture' button when ready...")
image_path = take_photo()

print("\nAnalyzing emotion...")
dominant_emotion, emotions = analyze_emotion(image_path)

if dominant_emotion:
    display_results(image_path, dominant_emotion, emotions)
else:
    print(f"Error: {emotions}")

# OPTION 2: Upload an image file
from google.colab import files

print("Upload an image with a face...")
uploaded = files.upload()

for filename in uploaded.keys():
    print(f"\nAnalyzing {filename}...")
    dominant_emotion, emotions = analyze_emotion(filename)

    if dominant_emotion:
        display_results(filename, dominant_emotion, emotions)
    else:
        print(f"Error: {emotions}")